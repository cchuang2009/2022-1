{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5b89f307-33c5-4d94-a3d4-11d34a4b7189",
      "metadata": {
        "id": "5b89f307-33c5-4d94-a3d4-11d34a4b7189"
      },
      "source": [
        "Bank Churn prediction\n",
        "===\n",
        "This lecture covers predicting customer churn behavior at a bank using supervised machine learning models. It walks through the full process from exploratory data analysis to feature engineering, model training, evaluation, and deployment.\n",
        "\n",
        "Some key points:\n",
        "---\n",
        "- Exploratory data analysis is done visualizing the data distributions and relationships using techniques like countplots.\n",
        "- Feature engineering steps include one-hot encoding categorical variables like country and label encoding for gender.\n",
        "- Several models are trained and evaluated: logistic regression, random forest, and XGBoost. Metrics like F1-score, recall, and confusion matrices are used.\n",
        "- An ensemble combining the logistic regression and XGBoost models is created and shows improved performance.\n",
        "- Final models are saved for deployment using pickle.\n",
        "- Code is provided to run the full pipeline on Google Colab using data from Kaggle.\n",
        "\n",
        "```\n",
        "How to face the Layoff flow in Market's trend\n",
        "\n",
        "If we are not being reused in the workplace, we can analyze customer data to understand the customer churn rate,\n",
        "thereby increasing our importance and indispensability.\n",
        "\n",
        "```\n",
        "\n",
        "Reference\n",
        "---\n",
        "\n",
        "1. [src](https://platform.stratascratch.com/data-projects/customer-churn-prediction)\n",
        "2. [src](https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset), from kaggle\n",
        "3. [Home work](https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb), [data](https://www.kaggle.com/datasets/sebastienverpile/consumercomplaintsdata)\n",
        "4. [Microsoft lecture](https://github.com/microsoft/fabric-samples/blob/main/docs-samples/data-science/ai-samples/python/AIsample%20-%20Bank%20Customer%20Churn.ipynb), [Lecture](https://github.com/microsoft/fabric-samples/blob/main/docs-samples/data-science/ai-samples/python/AIsample%20-%20Bank%20Customer%20Churn.ipynb)\n",
        "\n",
        "   \n",
        "[code](https://www.analyticsvidhya.com/blog/2022/09/bank-customer-churn-prediction-using-machine-learning/)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps\n",
        "---\n",
        "1. Load data,\n",
        "    - download data from Kaggle and saved in Google drive;\n",
        "    - load directly from Google drive;\n",
        "2. EDA, Exploratory Data analysis\n",
        "   - fill NaN if any\n",
        "   - Feature distributed\n",
        "3. Feature Engineering\n",
        "   - drop un-required features\n",
        "   - convert by standardization, normalization\n",
        "   - create new features\n",
        "   - convert categorical data to numerical data,\n",
        "   - binning numerical data binary\n",
        "   - encode non-numerical data\n",
        "   - data imbanlance\n",
        "4. modeling\n",
        "   - split data into train-test sets\n",
        "   - cv folding\n",
        "   - feature importance\n",
        "   - hyperparameters\n",
        "   - cross-validation\n",
        "   - implement\n",
        "\n",
        "5 ...    "
      ],
      "metadata": {
        "id": "J24zwByN6LTn"
      },
      "id": "J24zwByN6LTn"
    },
    {
      "cell_type": "markdown",
      "id": "c31fab92-65a4-4823-b9c0-42a64f4c18d0",
      "metadata": {
        "id": "c31fab92-65a4-4823-b9c0-42a64f4c18d0"
      },
      "source": [
        "Run in Google Drive, colab\n",
        "---\n",
        "\n",
        "1. Login google drive, add `colab` plugin\n",
        "2. create 2024 foloder, enter it and create data, named data; download data from Kaggle to it.\n",
        "3. create colab, open left menu and upload kaggle data intp `data`\n",
        "4. run"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# feature encoder\n",
        "!pip install -q category_encoders"
      ],
      "metadata": {
        "id": "EpEWP7HlQJI3"
      },
      "id": "EpEWP7HlQJI3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q -U imblearn\n",
        "!pip install -U imbalanced-learn"
      ],
      "metadata": {
        "id": "FZ5gmD99a33B"
      },
      "id": "FZ5gmD99a33B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q xgboost lightgbm catboost"
      ],
      "metadata": {
        "id": "lFKA-E9FeSS8"
      },
      "id": "lFKA-E9FeSS8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d74ead69-114b-408d-b979-ec330ceb63f9",
      "metadata": {
        "id": "d74ead69-114b-408d-b979-ec330ceb63f9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import gc\n",
        "from sklearn.metrics import classification_report, accuracy_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
        "\n",
        "from sklearn.utils import resample\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import category_encoders as ce\n",
        "\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "1 Load Data\n",
        "---"
      ],
      "metadata": {
        "id": "zl0SbXXFiR2A"
      },
      "id": "zl0SbXXFiR2A"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "0bTNhEs7qh4Q"
      },
      "id": "0bTNhEs7qh4Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05285f88-3574-4338-9447-1a6456feaa9e",
      "metadata": {
        "id": "05285f88-3574-4338-9447-1a6456feaa9e"
      },
      "outputs": [],
      "source": [
        "data= '/content/gdrive/MyDrive/2023/2023-2-Python-AI/data/archive.zip'\n",
        "df=pd.read_csv(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6038f79-8b2d-423d-8b38-a3449ab571a4",
      "metadata": {
        "id": "d6038f79-8b2d-423d-8b38-a3449ab571a4"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 Exploratory Data Analaysis, EDA\n",
        "---"
      ],
      "metadata": {
        "id": "OG8sWK3q_Ig7"
      },
      "id": "OG8sWK3q_Ig7"
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 numerical feathers, 2 object ones; and target is churn\n",
        "df.info()"
      ],
      "metadata": {
        "id": "poouYepqSU-r"
      },
      "id": "poouYepqSU-r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# good, no null cell\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "LAkT956A8X1U"
      },
      "id": "LAkT956A8X1U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rate of churn; too low to take care\n",
        "# only 20 % in churm\n",
        "\n",
        "df[['churn']].mean()"
      ],
      "metadata": {
        "id": "pR3j8uW0pE9A"
      },
      "id": "pR3j8uW0pE9A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# churn indepement of one's gender\n",
        "\n",
        "df.groupby(\"gender\").agg({\"churn\": [\"mean\",\"count\"]})"
      ],
      "metadata": {
        "id": "PmIGPV12VKAL"
      },
      "id": "PmIGPV12VKAL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# churn rate of Germany is igher than others\n",
        "\n",
        "df.groupby(\"country\").agg({\"churn\": [\"mean\",\"count\"]})"
      ],
      "metadata": {
        "id": "rHh3_QLvVbVy"
      },
      "id": "rHh3_QLvVbVy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  churn rate indepent of whether owns creredict card\n",
        "\n",
        "df.groupby(\"credit_card\").agg({\"churn\": [\"mean\",\"count\"]})"
      ],
      "metadata": {
        "id": "42EmfGKpVqlj"
      },
      "id": "42EmfGKpVqlj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# churn rate is lower if one registered from activity\n",
        "\n",
        "df.groupby(\"active_member\").agg({\"churn\": [\"mean\",\"count\"]})\n"
      ],
      "metadata": {
        "id": "FL89nKqoV_ru"
      },
      "id": "FL89nKqoV_ru",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the more one purchased, the more one is in churn state\n",
        "\n",
        "df.groupby(\"products_number\").agg({\"churn\": [\"mean\",\"count\"]})"
      ],
      "metadata": {
        "id": "XRRyaWCoWYzu"
      },
      "id": "XRRyaWCoWYzu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# no info?\n",
        "df.groupby(\"tenure\").agg({\"churn\": [\"mean\",\"count\"]})"
      ],
      "metadata": {
        "id": "z9uoo4X4gBI1"
      },
      "id": "z9uoo4X4gBI1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k8qmHQ4Gid5V"
      },
      "id": "k8qmHQ4Gid5V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kinds of Features\n",
        "---"
      ],
      "metadata": {
        "id": "QaDOEEvHHq3i"
      },
      "id": "QaDOEEvHHq3i"
    },
    {
      "cell_type": "code",
      "source": [
        "target= 'churn'\n",
        "categorical_variables = [col for col in df if col in \"O\"\n",
        "                         or df[col].nunique() <=10\n",
        "                         and col not in target]\n",
        "print(\"Categorical Features: \",categorical_variables)"
      ],
      "metadata": {
        "id": "R-dLPZryJcfz"
      },
      "id": "R-dLPZryJcfz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ymWs-O_2gaUX"
      },
      "id": "ymWs-O_2gaUX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_variables = [col for col in df.columns if df[col].dtype != \"object\"\n",
        "                        and df[col].nunique() >5]\n",
        "print(\"Numerical Features: \",numeric_variables)"
      ],
      "metadata": {
        "id": "-pnQCH1vK7qo"
      },
      "id": "-pnQCH1vK7qo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "id": "naC7mEUoLLLT"
      },
      "id": "naC7mEUoLLLT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python Syntax\n",
        "---\n",
        "1. Suppose that\n",
        "```\n",
        "X=[X0,X1,X2,...]\n",
        "```\n",
        "`enumerate(X)` display out:\n",
        "```\n",
        "0, X0\n",
        "1, X1\n",
        "...\n",
        "```\n",
        "\n",
        "2. create a sequence of pictues, `(m Rows X n Columns)` pictures:\n",
        "```python\n",
        "fig, axes = plt.subplots(nrows = m, ncols= n)\n",
        "sns.histplot(df[col], ax=axes[i][j]\n",
        "```\n"
      ],
      "metadata": {
        "id": "9YUo_KxB8ptP"
      },
      "id": "9YUo_KxB8ptP"
    },
    {
      "cell_type": "code",
      "source": [
        "# observe the numeric fertures what they are distributed, whether are instributed in Normal (Gaussian)\n",
        "# visualize kde density plot and check whether in Gaussian\n",
        "def kde_QQ_vis(df, cols):\n",
        "    sns.set_style('whitegrid')\n",
        "    # Create a figure and a set of subplots\n",
        "    fig, axes = plt.subplots(nrows=len(cols), ncols=2, figsize=(10, 3 * len(cols)))\n",
        "    for i, col in enumerate(cols):\n",
        "        # Histogram for the column\n",
        "        sns.histplot(df[col], kde=True, ax=axes[i][0], color='red', edgecolor='black')\n",
        "        axes[i][0].set_title(f'Histogram of {col}', fontsize=12)\n",
        "        axes[i][0].set_xlabel('')\n",
        "        axes[i][0].set_ylabel('Density')\n",
        "\n",
        "        # Q-Q plot for the column\n",
        "        stats.probplot(df[col], dist=\"norm\", plot=axes[i][1])\n",
        "        axes[i][1].set_title(f'Q-Q Plot of {col}', fontsize=12)\n",
        "        axes[i][1].set_xlabel('Theoretical Quantiles')\n",
        "        axes[i][1].set_ylabel('Ordered Values')\n",
        "        axes[i][1].get_lines()[0].set_color('blue')\n",
        "        axes[i][1].get_lines()[1].set_color('red')\n",
        "    # Adjust the layout\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "_8wChhSSZNkk"
      },
      "id": "_8wChhSSZNkk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check 'credit_score', 'balance', 'estimated_salary'\n",
        "kde_QQ_vis(df, numeric_variables)"
      ],
      "metadata": {
        "id": "fn6zpg6aa-nF"
      },
      "id": "fn6zpg6aa-nF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1UHDHBb7hoKw"
      },
      "id": "1UHDHBb7hoKw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "---\n",
        "The features, <b>`tenue`</b> and <b>`estimated_salary`</B>, are not distributed absolutely in Gaussian obviously!\n",
        "\n",
        "Data normalized\n",
        "---\n",
        "Nonamized data do help to modeling; different type\n",
        " different conversion:\n",
        " 1. sklearn.preprocessing.StandardScaler: Data in normal.\n",
        " $$\\bar X_i=\\frac{X_i-EX}{\\sigma (X)}\\sim N(0,1)$$\n",
        " 2. sklearn.preprocessing.MinMaxScaler: Data in abnomal\n",
        " $$ \\bar X_i=\\frac{X_i-X_\\min}{X_\\max-X_\\min}\\sim U(0,1)$$"
      ],
      "metadata": {
        "id": "K_ceEPi1cAxz"
      },
      "id": "K_ceEPi1cAxz"
    },
    {
      "cell_type": "code",
      "source": [
        "def StandardScaler(df,cols):\n",
        "    for col in cols:\n",
        "        df[col] = (df[col] - df[col].mean()) / df[col].std()\n",
        "    return df\n",
        "def NormalizationScaler(df,cols):\n",
        "    for col in cols:\n",
        "        df[col] = (df[col] - df[col].min()) / (df[col] .max()- df[col].min())\n",
        "    return df"
      ],
      "metadata": {
        "id": "_EP7haejcUOj"
      },
      "id": "_EP7haejcUOj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "kzvpMH7Hb_l-"
      },
      "id": "kzvpMH7Hb_l-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However numeric_variables[0]= `customer_id` redudent, each one owning unique one, give up."
      ],
      "metadata": {
        "id": "P63BH4rgLM26"
      },
      "id": "P63BH4rgLM26"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Washing, (Feature Engineering)\n",
        "---\n",
        "There are some packages that automatical do `Feature Engineering` work, suchas\n",
        "- [Featuretools](https://github.com/alteryx/featuretools),\n",
        "- [sklearn](https://scikit-learn.org/stable/)\n",
        "```python\n",
        "from sklearn.feature_selection import f_regression\n",
        "scores, _ = f_regression(df.iloc[:,0:2], df.iloc[:,-1])\n",
        "print(scores)\n",
        "```\n",
        "- [Feature-engine](https://feature-engine.trainindata.com/en/latest/)\n",
        "\n",
        "Here, we manually do these works:\n",
        "\n",
        "a). delete redundant features,`customer_id`<br>\n",
        "b)."
      ],
      "metadata": {
        "id": "vuv-yYpFNb_U"
      },
      "id": "vuv-yYpFNb_U"
    },
    {
      "cell_type": "code",
      "source": [
        "# drop un-required features\n",
        "def clean_data(df):\n",
        "    # Drop rows with missing data across all columns\n",
        "    df.dropna(inplace=True)\n",
        "    # Drop columns:  'customer_id',\n",
        "    df.drop(columns=['customer_id'], inplace=True)\n",
        "    return df\n",
        "\n",
        "df_clean = clean_data(df)"
      ],
      "metadata": {
        "id": "ITFu3F05NZpm"
      },
      "id": "ITFu3F05NZpm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.head(2)"
      ],
      "metadata": {
        "id": "Xk6a7pelERTW"
      },
      "id": "Xk6a7pelERTW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean=df.copy()\n",
        "df_clean.head(2)"
      ],
      "metadata": {
        "id": "fIHmP4qjPBn5"
      },
      "id": "fIHmP4qjPBn5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import f_regression\n"
      ],
      "metadata": {
        "id": "YRxiy4INxCzR"
      },
      "id": "YRxiy4INxCzR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sklean example\n",
        "# H0: the regression coefficient is equal to zero.\n",
        "# i.e. non-implantant freture to be used to predictctarget\n",
        "\n",
        "scores, p_vals = f_regression(df.iloc[:,4:9], df.iloc[:,-1])\n",
        "# Print the feature names and scores\n",
        "for i, score in enumerate(scores):\n",
        "    print(f\"Feature {i+4}: {df.columns[i+4]} - Score: {score:.3f}, with p-value: {p_vals[i]:.2f}\")"
      ],
      "metadata": {
        "id": "OW59jDF8JeBc"
      },
      "id": "OW59jDF8JeBc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.head(2)"
      ],
      "metadata": {
        "id": "eZ2wPA21yQwM"
      },
      "id": "eZ2wPA21yQwM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "# binning the data\n",
        "df_clean[\"CreditsScore\"] = pd.qcut(df_clean['credit_score'], 6, labels = [1, 2, 3, 4, 5, 6])\n",
        "df_clean[\"Age\"] = pd.qcut(df_clean['age'], 8, labels = [1, 2, 3, 4, 5, 6, 7, 8])\n",
        "df_clean[\"Balance\"] = pd.qcut(df_clean['balance'].rank(method=\"first\"), 5, labels = [1, 2, 3, 4, 5])\n",
        "df_clean[\"EstSalary\"] = pd.qcut(df_clean['estimated_salary'], 10, labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "df_clean[\"Tenure\"] = df_clean[\"tenure\"]/df_clean[\"age\"]\n",
        "df_clean.dtypes"
      ],
      "metadata": {
        "id": "50waHl56UQZ8"
      },
      "id": "50waHl56UQZ8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  data\n",
        "\n",
        "y_clean = df_clean[target]\n",
        "df_clean = df_clean.drop(target,axis=1)"
      ],
      "metadata": {
        "id": "3KhjN_s1EGwy"
      },
      "id": "3KhjN_s1EGwy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.dtypes"
      ],
      "metadata": {
        "id": "4QJitj1dDZ1Z"
      },
      "id": "4QJitj1dDZ1Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "We1iSv1adBid"
      },
      "id": "We1iSv1adBid",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.nunique()"
      ],
      "metadata": {
        "id": "zRwNB7WeDkZn"
      },
      "id": "zRwNB7WeDkZn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Feature value;\n",
        "# convert the last four categorical features to numeric ones\n",
        "features_=['CreditsScore','Age','Balance']\n",
        "for f_ in features_:\n",
        "    df_clean[f_] = df_clean[f_].astype(int)\n",
        "\n",
        "df_clean['EstSalary'] = df_clean['EstSalary'].astype(float)"
      ],
      "metadata": {
        "id": "NxeEpkRnOgmN"
      },
      "id": "NxeEpkRnOgmN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.dtypes"
      ],
      "metadata": {
        "id": "Bxka3H0cPmUH"
      },
      "id": "Bxka3H0cPmUH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: delete pandas column\n",
        "\n",
        "#df_clean.drop(columns=['age','tenure','balance','estimated_salary'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "C1MnlM79O6dt"
      },
      "id": "C1MnlM79O6dt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nornamize data\n",
        "---"
      ],
      "metadata": {
        "id": "oYRir34HT1av"
      },
      "id": "oYRir34HT1av"
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean=StandardScaler(df_clean,['age','balance','estimated_salary'])"
      ],
      "metadata": {
        "id": "f4u1uZtAVq8i"
      },
      "id": "f4u1uZtAVq8i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean= NormalizationScaler(df_clean,['tenure'])"
      ],
      "metadata": {
        "id": "ThefQgqfWG_-"
      },
      "id": "ThefQgqfWG_-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.head(2)"
      ],
      "metadata": {
        "id": "H8CRLm_GQ-TS"
      },
      "id": "H8CRLm_GQ-TS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.dtypes"
      ],
      "metadata": {
        "id": "c43ASIthRK_9"
      },
      "id": "c43ASIthRK_9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4y_S6z2PddLf"
      },
      "id": "4y_S6z2PddLf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features Combination\n",
        "---\n",
        "Create hybrid feature for two features:\n",
        "```\n",
        "df['new_feature']=df['col1'].astype(str)+'_'+df['col2'].astype(str)\n",
        "```\n",
        "the features, `cols` and `cols`, have to be treated as 'string'."
      ],
      "metadata": {
        "id": "hCDP8tUsdd6W"
      },
      "id": "hCDP8tUsdd6W"
    },
    {
      "cell_type": "code",
      "source": [
        "# combine features,string feafures only\n",
        "\n",
        "def feature_bind(df,col1,col2):\n",
        "    df[col1+'_'+col2]=df[col1].astype(str)+df[col2].astype(str)\n",
        "    return df"
      ],
      "metadata": {
        "id": "7zPHqsnIFQGB"
      },
      "id": "7zPHqsnIFQGB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Bis8gKOJeVUZ"
      },
      "id": "Bis8gKOJeVUZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean =  feature_bind(df_clean,'country','gender')\n",
        "df_clean =  feature_bind(df_clean,'country','products_number')\n",
        "df_clean =  feature_bind(df_clean,'country','Age')\n",
        "df_clean =  feature_bind(df_clean,'gender','products_number')\n",
        "df_clean =  feature_bind(df_clean,'gender','Age')\n",
        "df_clean =  feature_bind(df_clean,'products_number','Age')"
      ],
      "metadata": {
        "id": "vsgaTVvyEW1K"
      },
      "id": "vsgaTVvyEW1K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9NAvtt-ZecLh"
      },
      "id": "9NAvtt-ZecLh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GAT3gSQaHhxe"
      },
      "id": "GAT3gSQaHhxe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data saved\n",
        "\n",
        "saved='/content/gdrive/MyDrive/2023/2023-2-Python-AI/data/churn-data-05-24.csv'\n",
        "df_clean.to_csv(saved,index=False)"
      ],
      "metadata": {
        "id": "oz2rQwj_0GY6"
      },
      "id": "oz2rQwj_0GY6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "362f1ec7-2f40-473d-8b08-10d8b4473611",
      "metadata": {
        "id": "362f1ec7-2f40-473d-8b08-10d8b4473611"
      },
      "outputs": [],
      "source": [
        "# database diveded\n",
        "\n",
        "y = y_clean\n",
        "X = df_clean\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.head(2)"
      ],
      "metadata": {
        "id": "CFyUxI33Bg9y"
      },
      "id": "CFyUxI33Bg9y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering\n",
        "---\n",
        "1. create new features:<br>\n",
        "   a. 'gender'+'products_number'<br>\n",
        "   b. 'customer': 'country'+'gender'<br>\n",
        "   c. 'active_member'+'country'<br>\n",
        "   d. 'country'+'products_number'<br>"
      ],
      "metadata": {
        "id": "fhKciIdNX570"
      },
      "id": "fhKciIdNX570"
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "-zXU0auHug6P"
      },
      "id": "-zXU0auHug6P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llo_encoder(df,col,target=y):\n",
        "    df[col+'_llo_enc']=df[col]\n",
        "    loo_encoder = ce.LeaveOneOutEncoder(cols=[col+'_llo_enc'], sigma=0.05)\n",
        "    loo_encoder.fit(df, target)\n",
        "    X_encoded = loo_encoder.transform(df)\n",
        "    return X_encoded"
      ],
      "metadata": {
        "id": "tlL54mVsa2h9"
      },
      "id": "tlL54mVsa2h9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#obj_vars=df_clean.select_dtypes(include=['object']).columns\n",
        "#obj_vars"
      ],
      "metadata": {
        "id": "wesAzUbZ0nFD"
      },
      "id": "wesAzUbZ0nFD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "f_convert=['country_gender','country_products_number','country_Age']\n",
        "for col in f_convert:\n",
        "    X_enc=llo_encoder(X,col)\n",
        "    X=X_enc"
      ],
      "metadata": {
        "id": "37d__5pXNHCu"
      },
      "id": "37d__5pXNHCu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u5Rnn54100Lm"
      },
      "id": "u5Rnn54100Lm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head(2)"
      ],
      "metadata": {
        "id": "RoweP7JEOZRi"
      },
      "id": "RoweP7JEOZRi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "id": "iQZZrYGqbhMk"
      },
      "id": "iQZZrYGqbhMk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "79AX8W1xlZkT"
      },
      "id": "79AX8W1xlZkT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def target_encoder(df,col,target=y):\n",
        "    df[col+'_target_enc']=df[col]\n",
        "    target_encoder = ce.TargetEncoder(cols=[col+'_target_enc'])\n",
        "    target_encoder.fit(df, target)\n",
        "    X_encoded = target_encoder.transform(df)\n",
        "    return X_encoded"
      ],
      "metadata": {
        "id": "sOgWfwV_XXfo"
      },
      "id": "sOgWfwV_XXfo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "f_convert=['country_gender','country_products_number','country_Age']\n",
        "for col in f_convert:\n",
        "    X_enc=target_encoder(X,col)\n",
        "    X=X_enc"
      ],
      "metadata": {
        "id": "31BBHVyCXONz"
      },
      "id": "31BBHVyCXONz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3291d8dd-0eb1-4714-b9a2-40bafacfcef9",
      "metadata": {
        "id": "3291d8dd-0eb1-4714-b9a2-40bafacfcef9"
      },
      "outputs": [],
      "source": [
        "X.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def glmm_encoder(df,col,target=y):\n",
        "    df[col+'_glmm_enc']=df[col]\n",
        "    glmm_encoder = ce.GLMMEncoder(cols=col, binomial_target=True)\n",
        "    glmm_encoder.fit(df, target)\n",
        "    X_encoded = glmm_encoder.transform(df)\n",
        "    return X_encoded"
      ],
      "metadata": {
        "id": "8LwpyHRypS_E"
      },
      "id": "8LwpyHRypS_E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_convert=['country_gender','country_products_number','country_Age']\n",
        "for col in f_convert:\n",
        "    X_enc=glmm_encoder(X,col)\n",
        "    X=X_enc"
      ],
      "metadata": {
        "id": "GvNvv9olrRu6"
      },
      "id": "GvNvv9olrRu6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "id": "KkPMCMUjruir"
      },
      "id": "KkPMCMUjruir",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# binary\n",
        "\n",
        "X['gender']=X['gender'].apply(lambda x:1 if x=='Female' else 0)\n"
      ],
      "metadata": {
        "id": "4i6VPvfRVAQ4"
      },
      "id": "4i6VPvfRVAQ4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9GicGVw_iPMB"
      },
      "id": "9GicGVw_iPMB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.drop(columns=['country'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "seQuw8F_YDFE"
      },
      "id": "seQuw8F_YDFE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "id": "dxjXXm_quV-N"
      },
      "id": "dxjXXm_quV-N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I70jUxWPJmAd"
      },
      "id": "I70jUxWPJmAd"
    },
    {
      "cell_type": "code",
      "source": [
        "X.head(2)"
      ],
      "metadata": {
        "id": "80zKkjkiGZbW"
      },
      "id": "80zKkjkiGZbW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Imbalance\n",
        "---\n",
        "SMOTE + ENN\n",
        "\n",
        "`A.` SMOTE selected each sample from the minority samples successively as the root sample for the synthesis of the new sample. <BR>\n",
        "`B.` The following result was obtained by employing ENN to eliminate noise samples when the process of SMOTE is caused."
      ],
      "metadata": {
        "id": "6Hez3-FEVFyz"
      },
      "id": "6Hez3-FEVFyz"
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "id": "UvP98LJsYcLq"
      },
      "id": "UvP98LJsYcLq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imbalance set\n",
        "from imblearn.combine import SMOTEENN\n",
        "\n",
        "\n",
        "def imbance_set(X,y):\n",
        "    # Create the SMOTEENN object\n",
        "    sme = SMOTEENN(random_state=42)\n",
        "\n",
        "    # Fit and resample the data\n",
        "    X_resampled, y_resampled = sme.fit_resample(X, y)\n",
        "\n",
        "    # Counting churn values\n",
        "    churn_counts = y_resampled.value_counts()\n",
        "    print(churn_counts)\n",
        "    return X_resampled, y_resampled"
      ],
      "metadata": {
        "id": "40pxlZYSOp88"
      },
      "id": "40pxlZYSOp88",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "XNL9NJbvYAE4"
      },
      "id": "XNL9NJbvYAE4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,y=imbance_set(X,y)"
      ],
      "metadata": {
        "id": "XxD1BdKuCfhS"
      },
      "id": "XxD1BdKuCfhS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X)"
      ],
      "metadata": {
        "id": "5CSs9RZJO3Kh"
      },
      "id": "5CSs9RZJO3Kh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.columns"
      ],
      "metadata": {
        "id": "m2yk0sMIeSFh"
      },
      "id": "m2yk0sMIeSFh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features=list(X.columns)\n",
        "target = 'churn'"
      ],
      "metadata": {
        "id": "xDlYIdc-VMUm"
      },
      "id": "xDlYIdc-VMUm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model\n",
        "---\n",
        "database -> train set and test set"
      ],
      "metadata": {
        "id": "r0vnu6obVNIH"
      },
      "id": "r0vnu6obVNIH"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "gb1ch9vP5dXb"
      },
      "id": "gb1ch9vP5dXb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef5548f-d7f1-47d8-9439-e0a6d654b21c",
      "metadata": {
        "id": "0ef5548f-d7f1-47d8-9439-e0a6d654b21c"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "XGB_model = XGBClassifier()\n",
        "XGB_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "095f66c1-4405-4388-b039-d594bec56f58",
      "metadata": {
        "id": "095f66c1-4405-4388-b039-d594bec56f58"
      },
      "outputs": [],
      "source": [
        "y_pred = XGB_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "7RunhLE-9hr2"
      },
      "id": "7RunhLE-9hr2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_importances = pd.Series(XGB_model.feature_importances_, index = X_train.columns).sort_values(ascending = True)\n",
        "feat_importances.plot(kind = 'barh');"
      ],
      "metadata": {
        "id": "FUbXQPRQVpNJ"
      },
      "id": "FUbXQPRQVpNJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loo,target\n",
        "feat_importances = pd.Series(XGB_model.feature_importances_, index = X_train.columns).sort_values(ascending = True)\n",
        "feat_importances.plot(kind = 'barh');"
      ],
      "metadata": {
        "id": "32QQcYHGnF63"
      },
      "id": "32QQcYHGnF63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('test Accuracy:', accuracy)\n",
        "\n",
        "# Calculate the recall\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print('test Recall:', recall)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print('test F1 score:', f1)"
      ],
      "metadata": {
        "id": "QfkJxm6cNkZD"
      },
      "id": "QfkJxm6cNkZD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Banlance, Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('test Accuracy:', accuracy)\n",
        "\n",
        "# Calculate the recall\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print('test Recall:', recall)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print('test F1 score:', f1)"
      ],
      "metadata": {
        "id": "DcY6iBaUPLQp"
      },
      "id": "DcY6iBaUPLQp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Banlance, Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('test Accuracy:', accuracy)\n",
        "\n",
        "# Calculate the recall\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print('test Recall:', recall)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print('test F1 score:', f1)"
      ],
      "metadata": {
        "id": "-LteXjnMPYRC"
      },
      "id": "-LteXjnMPYRC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-Validtion\n",
        "---"
      ],
      "metadata": {
        "id": "X9T-TU4lwvNR"
      },
      "id": "X9T-TU4lwvNR"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n"
      ],
      "metadata": {
        "id": "LW-ws4tFw0WL"
      },
      "id": "LW-ws4tFw0WL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "XGB_model = XGBClassifier()\n",
        "\n",
        "# Define the cross-validation strategy, for example, Stratified K-Folds\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(XGB_model, X_train, y_train, cv=cv_strategy, scoring='accuracy')\n"
      ],
      "metadata": {
        "id": "Kjw3dyKtw0Z_"
      },
      "id": "Kjw3dyKtw0Z_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "# If you want to fit the model on the entire training data after cross-validation\n",
        "XGB_model.fit(X_train, y_train)\n",
        "y_pred = XGB_model.predict(X_test)"
      ],
      "metadata": {
        "id": "yX2LEbWTw0dI"
      },
      "id": "yX2LEbWTw0dI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation and Visulization\n",
        "---\n",
        "use xgboost model as a template hew"
      ],
      "metadata": {
        "id": "tR8EgBpNxZ4d"
      },
      "id": "tR8EgBpNxZ4d"
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import plot_tree\n",
        "\n",
        "# Plot the first tree\n",
        "plt.figure(figsize=(20, 20))\n",
        "plot_tree(XGB_model, num_trees=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RXlA309Nw0gS"
      },
      "id": "RXlA309Nw0gS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q shap"
      ],
      "metadata": {
        "id": "GbPqCB9exoCl"
      },
      "id": "GbPqCB9exoCl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "# Initialize the SHAP explainer\n",
        "explainer = shap.Explainer(XGB_model, X_train)\n",
        "shap_values = explainer(X_train)\n",
        "\n",
        "# Summary plot\n",
        "shap.summary_plot(shap_values, X_train)\n",
        "\n",
        "# Dependence plot for a specific feature\n",
        "shap.dependence_plot(\"feature_name\", shap_values, X_train)\n",
        "\n",
        "# Force plot for a single prediction\n",
        "shap.force_plot(explainer.expected_value, shap_values[0], X_train.iloc[0])"
      ],
      "metadata": {
        "id": "jkeQfxtoxoF5"
      },
      "id": "jkeQfxtoxoF5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# $learning curve\n",
        "\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "train_sizes, train_scores, test_scores = learning_curve(XGB_model, X_train, y_train, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
        "\n",
        "# Calculate mean and standard deviation\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "# Plot learning curve\n",
        "plt.figure()\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "plt.xlabel(\"Training examples\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q7j38HuNxzbh"
      },
      "id": "q7j38HuNxzbh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "y_pred_proba = XGB_model.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "udxkPSDnxzec"
      },
      "id": "udxkPSDnxzec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coffusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "y_pred = XGB_model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zFEAOH2RyPru"
      },
      "id": "zFEAOH2RyPru",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bKhnaOkEyPvK"
      },
      "id": "bKhnaOkEyPvK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hfi1kTGwVzTA"
      },
      "id": "Hfi1kTGwVzTA"
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "metadata": {
        "id": "Mv3zfsMKVpRU"
      },
      "id": "Mv3zfsMKVpRU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a neural network model\n",
        "k_model = Sequential()\n",
        "k_model.add(Dense(11, activation='relu', input_shape=(df.shape[1],)))\n",
        "k_model.add(Dense(4, activation='relu'))\n",
        "k_model.add(Dense(1, activation='sigmoid'))\n",
        "k_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "dliqStUoO_c-"
      },
      "id": "dliqStUoO_c-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Keras model\n",
        "k_model = Sequential()\n",
        "k_model.add(Dense(64, activation='relu', input_shape=(X.shape[1],)))\n",
        "k_model.add(Dense(32, activation='relu'))\n",
        "k_model.add(Dense(1, activation='sigmoid'))\n",
        "k_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the training data\n",
        "k_model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "9B3HdTPaO_pL"
      },
      "id": "9B3HdTPaO_pL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the testing data\n",
        "y_pred = k_model.predict(X_test)\n",
        "\n",
        "# Convert the predictions to binary labels\n",
        "y_pred = (y_pred > 0.4).astype(int)\n"
      ],
      "metadata": {
        "id": "hdk05yfiQ22Q"
      },
      "id": "hdk05yfiQ22Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('test Accuracy:', accuracy)\n",
        "\n",
        "# Calculate the recall\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print('test Recall:', recall)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print('test F1 score:', f1)"
      ],
      "metadata": {
        "id": "vTmKtGfgRZ8g"
      },
      "id": "vTmKtGfgRZ8g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('test Accuracy:', accuracy)\n",
        "\n",
        "# Calculate the recall\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print('test Recall:', recall)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print('test F1 score:', f1)"
      ],
      "metadata": {
        "id": "ngwcfp7eRaBJ"
      },
      "id": "ngwcfp7eRaBJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metrics\n",
        "---\n",
        "\n",
        "1. `Precision`: Precision is the ratio of true positives (TP) to the sum of true positives and false positives (FP):\n",
        "Precision = TP / (TP + FP)\n",
        "\n",
        "2. `Recall`: Recall is the ratio of true positives (TP) to the sum of true positives and false negatives (FN):\n",
        "Recall = TP / (TP + FN)\n",
        "\n",
        "3. `F1`-score: The F1-score is the harmonic mean of precision and recall:\n",
        "$$F1 = 2  \\frac{\\text{Precision}\\times \\text{Recall}}{\\text{Precision + Recall}}$$"
      ],
      "metadata": {
        "id": "-1EBbOOIW2fP"
      },
      "id": "-1EBbOOIW2fP"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FUnZSM_TVpW0"
      },
      "id": "FUnZSM_TVpW0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa7041c4-9a4f-4ed9-9081-2433035204a0",
      "metadata": {
        "id": "aa7041c4-9a4f-4ed9-9081-2433035204a0"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred))\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "#print(cnf_matrix)\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "#print(cnf_matrix)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "5GENvhgfWEJy"
      },
      "id": "5GENvhgfWEJy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loo, GLMM, target, country_products_number\n",
        "print(classification_report(y_test, y_pred))\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "#print(cnf_matrix)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "lPIbkj8enYIA"
      },
      "id": "lPIbkj8enYIA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loo,  country_products_number\n",
        "print(classification_report(y_test, y_pred))\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "#print(cnf_matrix)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "CK4PgTQbIUQc"
      },
      "id": "CK4PgTQbIUQc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f523e2e9-3f5a-484a-8e11-5c65ab2a28b4",
      "metadata": {
        "id": "f523e2e9-3f5a-484a-8e11-5c65ab2a28b4"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=plt.cm.summer):\n",
        "    plt.clf\n",
        "    plt.imshow(cm, interpolation='nearest')\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(target_names))\n",
        "    plt.xticks(tick_marks, target_names, rotation=45)\n",
        "    plt.yticks(tick_marks, target_names)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    width, height = cm.shape\n",
        "\n",
        "    for x in range(width):\n",
        "        for y in range(height):\n",
        "            plt.annotate(str(cm[x][y]), xy=(y, x),\n",
        "                        horizontalalignment='center',\n",
        "                        verticalalignment='center',fontsize=22)\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8424a22b-26f0-427c-99df-5a60d48110b9",
      "metadata": {
        "id": "8424a22b-26f0-427c-99df-5a60d48110b9"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(cnf_matrix, np.unique(y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(cnf_matrix, np.unique(y_pred))"
      ],
      "metadata": {
        "id": "avTnWSj1WMUo"
      },
      "id": "avTnWSj1WMUo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0ce2a8d-c9c0-46ec-845c-7cbf0c5951a2",
      "metadata": {
        "id": "a0ce2a8d-c9c0-46ec-845c-7cbf0c5951a2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 20)\n",
        "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "accuracies = cross_val_score(estimator = XGB_model, X = X_train, y = y_train, cv = 20)\n",
        "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
      ],
      "metadata": {
        "id": "0N2nCLC5WYK5"
      },
      "id": "0N2nCLC5WYK5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "accuracies = cross_val_score(estimator = XGB_model, X = X_test, y = y_test, cv = 20)\n",
        "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
      ],
      "metadata": {
        "id": "6cwhmYVFWz7i"
      },
      "id": "6cwhmYVFWz7i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classification, rfc\n",
        "---\n",
        "Set of week models"
      ],
      "metadata": {
        "id": "RT-4KgXxTgf6"
      },
      "id": "RT-4KgXxTgf6"
    },
    {
      "cell_type": "code",
      "source": [
        "rfc1= RandomForestClassifier(max_depth=4, max_features=4, min_samples_split=3, random_state=1) # Pass hyperparameters\n",
        "\n",
        "rfc1.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "OHk6mz7VYPXe"
      },
      "id": "OHk6mz7VYPXe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_feature_importances(model):\n",
        "    feat_importances = pd.Series(model.feature_importances_, index = X_train.columns).sort_values(ascending = True)\n",
        "    feat_importances.plot(kind = 'barh');\n",
        "    #plt.barh(range(n_features), model.feature_importances_, align='center')\n",
        "    #plt.yticks(np.arange(n_features), X.columns)"
      ],
      "metadata": {
        "id": "PEE17ABYUsmz"
      },
      "id": "PEE17ABYUsmz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_feature_importances(rfc1)"
      ],
      "metadata": {
        "id": "7TMnCVxHUQUd"
      },
      "id": "7TMnCVxHUQUd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the accuracy\n",
        "y_pred = rfc1.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('test Accuracy:', accuracy)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print('test F1 score:', f1)"
      ],
      "metadata": {
        "id": "1b5RKTSdUk31"
      },
      "id": "1b5RKTSdUk31",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_model(model, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test):\n",
        "    #model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print('test Accuracy:', accuracy)\n",
        "\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    print('test Recall:', recall)\n",
        "\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    print('test F1 score:', f1)\n",
        "\n",
        "#score_model(rfc1)"
      ],
      "metadata": {
        "id": "TDCdxH2IWJ5h"
      },
      "id": "TDCdxH2IWJ5h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_model(XGB_model)"
      ],
      "metadata": {
        "id": "np8ivdPmXSs1"
      },
      "id": "np8ivdPmXSs1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc2= RandomForestClassifier(max_depth=8, max_features=6, min_samples_split=3, random_state=42) # Pass hyperparameters\n",
        "rfc2.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "0Bcgl2OeXgTD"
      },
      "id": "0Bcgl2OeXgTD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_model(rfc2)"
      ],
      "metadata": {
        "id": "2wgHgRTGXp2p"
      },
      "id": "2wgHgRTGXp2p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importance\n",
        "importances = rfc.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in rfc.estimators_], axis=0)\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
        "plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\n",
        "plt.xlim([-1, X_train.shape[1]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7MrrXb9UzWqu"
      },
      "id": "7MrrXb9UzWqu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tree Viz\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "# Plot the first tree in the forest\n",
        "plt.figure(figsize=(20, 20))\n",
        "plot_tree(rfc.estimators_[0], feature_names=X_train.columns, filled=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FgUdJalmzWwV"
      },
      "id": "FgUdJalmzWwV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the SHAP explainer\n",
        "explainer = shap.Explainer(rfc, X_train)\n",
        "shap_values = explainer(X_train)\n",
        "\n",
        "# Summary plot\n",
        "shap.summary_plot(shap_values, X_train)\n",
        "\n",
        "# Dependence plot for a specific feature\n",
        "shap.dependence_plot(\"feature_name\", shap_values, X_train)\n",
        "\n",
        "# Force plot for a single prediction\n",
        "shap.force_plot(explainer.expected_value, shap_values[0], X_train.iloc[0])"
      ],
      "metadata": {
        "id": "WL2ny7szzhSi"
      },
      "id": "WL2ny7szzhSi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sizes, train_scores, test_scores = learning_curve(rfc, X_train, y_train, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
        "\n",
        "# Calculate mean and standard deviation\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np"
      ],
      "metadata": {
        "id": "TYHf7TeAzhgc"
      },
      "id": "TYHf7TeAzhgc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "train_sizes, train_scores, test_scores = learning_curve(rfc, X_train, y_train, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
        "\n",
        "# Calculate mean and standard deviation\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "# Plot learning curve\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.xlabel(\"Training examples\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.grid()\n",
        "\n",
        "# Plot the mean and standard deviation for training and validation scores\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NLgR_Aynzmh-"
      },
      "id": "NLgR_Aynzmh-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "y_pred_proba = rfc.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QdM2AXUU00MW"
      },
      "id": "QdM2AXUU00MW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vp_pH9oF00P-"
      },
      "id": "Vp_pH9oF00P-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DMBsqnkY00Ts"
      },
      "id": "DMBsqnkY00Ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "go1TO8Mbzmlv"
      },
      "id": "go1TO8Mbzmlv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_model = LGBMClassifier(learning_rate = 0.07,\n",
        "                        max_delta_step = 2,\n",
        "                        n_estimators = 100,\n",
        "                        max_depth = 10,\n",
        "                        eval_metric = \"logloss\",\n",
        "                        objective='binary',\n",
        "                        random_state=42)\n",
        "lgbm_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "roaWbVP8X2ml"
      },
      "id": "roaWbVP8X2ml",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_model(lgbm_model)"
      ],
      "metadata": {
        "id": "w7ifiWciEGCv"
      },
      "id": "w7ifiWciEGCv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "XGB_model2 = XGBClassifier(\n",
        "    learning_rate=0.07,\n",
        "    max_delta_step=2,\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    eval_metric=\"logloss\",\n",
        "    objective=\"binary:logistic\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    booster=\"gbtree\",\n",
        "    nthread=4,\n",
        "    gamma=0,\n",
        "    min_child_weight=1,\n",
        "    subsample=1,\n",
        "    colsample_bytree=1,\n",
        "    reg_alpha=0,\n",
        "    reg_lambda=1,\n",
        "    scale_pos_weight=1,\n",
        "    base_score=0.5,\n",
        "    missing=0,\n",
        "    num_parallel_tree=1,\n",
        "    predictor=\"auto\",\n",
        "    max_leaves=31,\n",
        "    tree_method=\"hist\",\n",
        "    gpu_id=-1,\n",
        "    enable_categorical=False,\n",
        "    validate_parameters=True,\n",
        "    disable_default_eval_metric=False,\n",
        "    grow_policy=\"depthwise\",\n",
        "    max_bin=255,\n",
        "    min_data_in_leaf=1,\n",
        "    min_child_samples=20,\n",
        "    max_depth_reduction=0,\n",
        "    min_data_in_bin=1,\n",
        "    min_gain_to_split=0,\n",
        "    min_split_gain=0,\n",
        "    min_split_loss=0,\n",
        "    )\n",
        "\n",
        "XGB_model2.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "gUEMgsaCYRZJ"
      },
      "id": "gUEMgsaCYRZJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_model(XGB_model2)"
      ],
      "metadata": {
        "id": "Jz7eVHE_b2X2"
      },
      "id": "Jz7eVHE_b2X2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_model(XGB_model2)"
      ],
      "metadata": {
        "id": "MO66nwiiIy5Y"
      },
      "id": "MO66nwiiIy5Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_model(XGB_model2)"
      ],
      "metadata": {
        "id": "LmaBUMBwE3fp"
      },
      "id": "LmaBUMBwE3fp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ce."
      ],
      "metadata": {
        "id": "yXVQnahOaXWd"
      },
      "id": "yXVQnahOaXWd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_model=CatBoostClassifier(learning_rate=0.03,silent=True)\n",
        "cat_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "2cU3-gFudNgX"
      },
      "id": "2cU3-gFudNgX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_model(cat_model)"
      ],
      "metadata": {
        "id": "Cusp4jbAdPHs"
      },
      "id": "Cusp4jbAdPHs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfm = confusion_matrix(y_test, y_pred=cat_model.predict(X_test))\n",
        "plot_confusion_matrix(cfm, classes=['Non Churn','Churn'],\n",
        "                      title='CatBoost Prediction')\n",
        "tn, fp, fn, tp = cfm.ravel()"
      ],
      "metadata": {
        "id": "HyuPard4eUuD"
      },
      "id": "HyuPard4eUuD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plot_confusion_matrix(cfm, ['Non Churn','Churn'],title='Catboost')\n"
      ],
      "metadata": {
        "id": "pPgo_042fJb-"
      },
      "id": "pPgo_042fJb-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0E1ZP_2MgNbo"
      },
      "id": "0E1ZP_2MgNbo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble of Models\n",
        "---"
      ],
      "metadata": {
        "id": "FH1YV48BYQDX"
      },
      "id": "FH1YV48BYQDX"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y3jHCYgvf-zm"
      },
      "id": "Y3jHCYgvf-zm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_models = {\n",
        "    'logistic_regression' : LogisticRegression(random_state = 42, max_iter = 10000),\n",
        "    'Random_forest' : RandomForestClassifier(n_estimators = 150, max_depth = 4, random_state = 42),\n",
        "    \"XGBoost\" : xgb.XGBClassifier(n_estimators = 200, max_depth = 5, random_state = 42)\n",
        "}"
      ],
      "metadata": {
        "id": "wTEStw20RyTB"
      },
      "id": "wTEStw20RyTB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_train_scores = []\n",
        "f1_test_scores = []\n",
        "recall_test_scores = []\n",
        "model_names = list(list_of_models.keys())"
      ],
      "metadata": {
        "id": "LRiSJaxpRyXX"
      },
      "id": "LRiSJaxpRyXX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_names"
      ],
      "metadata": {
        "id": "nrftaC3sZaxG"
      },
      "id": "nrftaC3sZaxG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in model_names:\n",
        "    print(\"\\nFor Model:\", model)\n",
        "\n",
        "    list_of_models[model].fit(X_train, y_train)\n",
        "\n",
        "    print(\"\\nFor Training Set:\")\n",
        "\n",
        "    y_train_pred = list_of_models[model].predict(X_train)\n",
        "\n",
        "    f1_train = f1_score(y_train, y_train_pred, average='macro')\n",
        "    print(\"\\nMacro F1 Score:\", f1_train)\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    Confusion_Matrix = confusion_matrix(y_train.values, y_train_pred)\n",
        "    cm_display = ConfusionMatrixDisplay(confusion_matrix = Confusion_Matrix, display_labels = [False, True])\n",
        "    cm_display.plot()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"For Test Set:\")\n",
        "\n",
        "    y_test_pred = list_of_models[model].predict(X_test)\n",
        "\n",
        "    f1_test = f1_score(y_test, y_test_pred, average='macro')\n",
        "    print(\"\\nMacro F1 Score:\", f1_test)\n",
        "\n",
        "    recall_test_score = recall_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    Confusion_Matrix = confusion_matrix(y_test, y_test_pred)\n",
        "    cm_display = ConfusionMatrixDisplay(confusion_matrix = Confusion_Matrix, display_labels = [False, True])\n",
        "    cm_display.plot()\n",
        "    plt.show()\n",
        "\n",
        "    f1_train_scores.append(f1_train)\n",
        "    f1_test_scores.append(f1_test)\n",
        "    recall_test_scores.append(recall_test_score)"
      ],
      "metadata": {
        "id": "W3pGyKdaRybO"
      },
      "id": "W3pGyKdaRybO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model_names           f1_train_scores  f1_test_scores   recall_test_scores\")\n",
        "for i in range(len(model_names)):\n",
        "    print(f\"{model_names[i]:<22} {f1_train_scores[i]:.6f}          {f1_test_scores[i]:.6f}.          {recall_test_scores[i]:.6f}\")"
      ],
      "metadata": {
        "id": "mrVa_dXahCZy"
      },
      "id": "mrVa_dXahCZy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_score=pd.DataFrame()\n",
        "df_score['model']=model_names\n",
        "df_score['f1_train_scores']=f1_train_scores\n",
        "df_score['f1_test_scores']=f1_test_scores\n",
        "df_score['recall_test_scores']=recall_test_scores\n",
        "df_score"
      ],
      "metadata": {
        "id": "-qjLD9CJjH_M"
      },
      "id": "-qjLD9CJjH_M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BEv885L8Z8yv"
      },
      "id": "BEv885L8Z8yv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to table\n",
        "from IPython.display import HTML\n",
        "HTML(df_score.to_html())"
      ],
      "metadata": {
        "id": "XAXmqnIWZK4l"
      },
      "id": "XAXmqnIWZK4l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_reg_model = LogisticRegression(random_state = 42, max_iter = 10000)\n",
        "xgb_model = xgb.XGBClassifier(n_estimators = 200, max_depth = 5, random_state = 42)\n",
        "\n",
        "#fit logistic regression model\n",
        "log_reg_model.fit(X, y)\n",
        "#fit xgb model\n",
        "xgb_model.fit(X, y)\n",
        "\n",
        "#predict using ensemble of both the models\n",
        "y_pred_proba_log = [x[1] for x in log_reg_model.predict_proba(X)]\n",
        "y_pred_proba_xgb = [x[1] for x in xgb_model.predict_proba(X)]\n",
        "y_pred_proba_log = np.array(y_pred_proba_log)\n",
        "y_pred_proba_xgb = np.array(y_pred_proba_xgb)\n",
        "\n",
        "y_pred_proba = (y_pred_proba_log + y_pred_proba_xgb) / 2.0\n",
        "\n",
        "y_pred = np.where(y_pred_proba > 0.4, 1, 0)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9MObqdtrkSOs"
      },
      "id": "9MObqdtrkSOs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = f1_score(y, y_pred, average = 'macro')\n",
        "print(\"\\nMacro F1 Score:\", f1)\n",
        "\n",
        "recall = recall_score(y, y_pred, average = 'macro')\n",
        "print(\"\\nMacro Recall Score:\", recall)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "Confusion_Matrix = confusion_matrix(y, y_pred)\n",
        "cm_display = ConfusionMatrixDisplay(confusion_matrix = Confusion_Matrix, display_labels = [False, True])\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c6KErk3_ks6E"
      },
      "id": "c6KErk3_ks6E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save and export models to be used in deployment\n",
        "import pickle\n",
        "#Save and export models to be used in deployment\n",
        "\n",
        "pickle.dump(log_reg_model, open('churn_logistic_regression_model_for_deployment.pkl','wb'))\n",
        "pickle.dump(xgb_model, open('churn_xgb_model_for_deployment.pkl','wb'))"
      ],
      "metadata": {
        "id": "0CMw4OY-lEbQ"
      },
      "id": "0CMw4OY-lEbQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "icpbCqeZaCg8"
      },
      "id": "icpbCqeZaCg8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Be5pvd2ob1kV"
      },
      "id": "Be5pvd2ob1kV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head(5)"
      ],
      "metadata": {
        "id": "2dVyyue-mOFM"
      },
      "id": "2dVyyue-mOFM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.columns"
      ],
      "metadata": {
        "id": "CgZQpnhInYdz"
      },
      "id": "CgZQpnhInYdz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imbalance Problem\n",
        "---\n",
        "Gerenally, churn data is a little small such that affects the prediction. Here we can enlarge the churn dataset by simulation, and re-make the prediction model:\n",
        "\n",
        "```python\n",
        "X = df_data_model.loc[:, df_data_model.columns!=’churn’]\n",
        "y = df_data_model[‘churn’]\n",
        "\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE()\n",
        "# fit predictor and target variable\n",
        "X_smote, y_smote = smote.fit_resample(X,y)\n",
        "print(‘Original dataset shape’, Counter(y))\n",
        "print(‘Resample dataset shape’, Counter(y_smote))\n",
        "\n",
        "# Break off validation set from training data\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_smote, y_smote, train_size=0.7, test_size=0.3, random_state=0)\n",
        "# summarize\n",
        "print(‘Train’, X_train.shape, y_train.shape)\n",
        "print(‘Test’, X_valid.shape, y_valid.shape)\n",
        "\n",
        "#adaboost model training\n",
        "ada_clf = AdaBoostClassifier(random_state=0)\n",
        "kfold = KFold(n_splits = 10, random_state = 5)\n",
        "results = cross_val_score(ada_clf, X_train, y_train, cv = kfold)\n",
        "print(results.mean())\n",
        "\n",
        "\n",
        "#train model\n",
        "ada_clf.fit(X_train, y_train)\n",
        "#make predictions\n",
        "y_pred = ada_clf.predict(X_valid)\n",
        "#metrics\n",
        "print('Model accuracy score: ',accuracy_score(y_valid,y_pred))\n",
        "print('Confusion matrix: ')\n",
        "print(confusion_matrix(y_valid,y_pred))\n",
        "print(classification_report(y_valid,y_pred))\n",
        "```"
      ],
      "metadata": {
        "id": "70DEuF07lrsH"
      },
      "id": "70DEuF07lrsH"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MVgt5Hxplql-"
      },
      "id": "MVgt5Hxplql-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appendix, Pycaret Review\n",
        "---"
      ],
      "metadata": {
        "id": "9UT0x7n-Pun-"
      },
      "id": "9UT0x7n-Pun-"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pycaret"
      ],
      "metadata": {
        "id": "GN33Z5E4PtE6"
      },
      "id": "GN33Z5E4PtE6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycaret.classification import *"
      ],
      "metadata": {
        "id": "sgkSXmFiPtsB"
      },
      "id": "sgkSXmFiPtsB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X['churn']=y"
      ],
      "metadata": {
        "id": "OUE_I0kOWANa"
      },
      "id": "OUE_I0kOWANa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head(2)"
      ],
      "metadata": {
        "id": "dgxJFb_JWRZG"
      },
      "id": "dgxJFb_JWRZG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = setup(data=X,target='churn',session_id=123)\n",
        "best = compare_models()"
      ],
      "metadata": {
        "id": "gRoP_eIhPt1U"
      },
      "id": "gRoP_eIhPt1U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head(2)"
      ],
      "metadata": {
        "id": "Tdc5Q56VWb3L"
      },
      "id": "Tdc5Q56VWb3L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improvements\n",
        "---\n",
        "- Using SHAP to analyze the key factors driving churn predictions from the XGBoost model\n",
        "- Identifying high-risk customers based on a probability threshold\n",
        "- Implementing a function to regularly score the full customer base and save high-risk cases\n",
        "- Setting up the scoring function to run as a recurring monthly job"
      ],
      "metadata": {
        "id": "EEYUU-d1iG8q"
      },
      "id": "EEYUU-d1iG8q"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q shap"
      ],
      "metadata": {
        "id": "aE9Wl5I2ix9M"
      },
      "id": "aE9Wl5I2ix9M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Root Cause Analysis\n",
        "import shap\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "wUFYPfhXireZ"
      },
      "id": "wUFYPfhXireZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load trained XGBoost model\n",
        "xgb_model = pickle.load(open('churn_xgb_model.pkl', 'rb'))\n",
        "\n",
        "# Sample customer data\n",
        "customer_data = X.sample(100)\n",
        "\n",
        "# Compute SHAP values to understand feature importance\n",
        "explainer = shap.TreeExplainer(xgb_model)\n",
        "shap_values = explainer.shap_values(customer_data)\n",
        "\n",
        "# Visualize top factors driving churn predictions\n",
        "shap.summary_plot(shap_values, customer_data, plot_type=\"bar\")\n",
        "plt.show()\n",
        "\n",
        "# Churn Prevention\n",
        "# Identify high-risk customers based on churn probability threshold\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Compute probability predictions\n",
        "y_prob = xgb_model.predict_proba(X)[:,1]\n",
        "\n",
        "# Compute false positive and true positive rates\n",
        "fpr, tpr, thresholds = roc_curve(y, y_prob)\n",
        "\n",
        "# Find threshold for 80% tpr\n",
        "tpr80 = 0.8\n",
        "idx = np.argwhere(tpr >= tpr80)[0]\n",
        "threshold = thresholds[idx]\n",
        "\n",
        "# Customers above threshold are high-risk\n",
        "high_risk_customers = X[y_prob >= threshold]\n",
        "\n",
        "print(f\"Number of high-risk customers: {len(high_risk_customers)}\")\n",
        "\n",
        "# Monitoring System\n",
        "import datetime\n",
        "\n",
        "# Score full customer base each month\n",
        "def score_customer_base(date):\n",
        "    customers = get_all_customers(date)\n",
        "    customers = preprocess_data(customers)\n",
        "    scores = xgb_model.predict_proba(customers)[:,1]\n",
        "    high_risk = customers[scores >= threshold]\n",
        "\n",
        "    # Save high-risk cases for further action\n",
        "    high_risk.to_csv(f\"high_risk_{date}.csv\")\n",
        "\n",
        "# Set up monitoring job to run monthly\n",
        "next_run = datetime.datetime.today() + datetime.timedelta(days=30)\n",
        "score_customer_base(next_run)"
      ],
      "metadata": {
        "id": "4h5iVYtAWb7J"
      },
      "id": "4h5iVYtAWb7J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c1bXlUI4i52q"
      },
      "id": "c1bXlUI4i52q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d3baed02-5a2f-4801-b4f4-79f59953f2ec",
      "metadata": {
        "id": "d3baed02-5a2f-4801-b4f4-79f59953f2ec"
      },
      "source": [
        "Homework\n",
        "---\n",
        "Whatever method was used, `Domain Expertise` is the the important factor where the decision was made."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# gdrive/MyDrive/2023/2023-2-Python-AI"
      ],
      "metadata": {
        "id": "wua-Xjtg0HUC"
      },
      "id": "wua-Xjtg0HUC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls gdrive/MyDrive/2023/2023-2-Python-AI/data"
      ],
      "metadata": {
        "id": "ni4gSx190pe7"
      },
      "id": "ni4gSx190pe7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dc991c6-647e-468e-a270-8c713a66dd39",
      "metadata": {
        "id": "0dc991c6-647e-468e-a270-8c713a66dd39"
      },
      "outputs": [],
      "source": [
        "# Very complainted in the second issue\n",
        "import pandas as pd\n",
        "df = pd.read_csv('gdrive/MyDrive/2023/2023-2-Python-AI/data/Consumer_Complaints.csv.zip')\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61bc6cd3-b61d-4d18-9c3f-167243a43740",
      "metadata": {
        "id": "61bc6cd3-b61d-4d18-9c3f-167243a43740"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f50ff69-f957-4f00-8e2f-d06be2bba818",
      "metadata": {
        "id": "7f50ff69-f957-4f00-8e2f-d06be2bba818"
      },
      "outputs": [],
      "source": [
        "# delete the feature which is absent too many items\n",
        "df = df[pd.notnull(df['Consumer complaint narrative'])]\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06e8a670-e01a-400d-8309-6ea0229dd4dd",
      "metadata": {
        "id": "06e8a670-e01a-400d-8309-6ea0229dd4dd"
      },
      "outputs": [],
      "source": [
        "df = df[['Product', 'Consumer complaint narrative']]\n",
        "df['category_id'] = df['Product'].factorize()[0]\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39779d76-47d1-479a-8f4b-27dcab5a4fc0",
      "metadata": {
        "id": "39779d76-47d1-479a-8f4b-27dcab5a4fc0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "df.groupby('Product')['Consumer complaint narrative'].count().plot.bar(ylim=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67efaa7f-9adc-4f9f-a283-033d3754d175",
      "metadata": {
        "id": "67efaa7f-9adc-4f9f-a283-033d3754d175"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
        "\n",
        "features = tfidf.fit_transform(df['Consumer complaint narrative']).toarray()\n",
        "labels = df.category_id\n",
        "features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df51612d-fb02-4dd1-9be7-89518122c245",
      "metadata": {
        "id": "df51612d-fb02-4dd1-9be7-89518122c245"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5de9a36-60b5-446e-9bfa-b9b4dc731907",
      "metadata": {
        "id": "b5de9a36-60b5-446e-9bfa-b9b4dc731907"
      },
      "outputs": [],
      "source": [
        "from io import StringIO\n",
        "category_id_df = df[['Product', 'category_id']].drop_duplicates().sort_values('category_id')\n",
        "category_to_id = dict(category_id_df.values)\n",
        "id_to_category = dict(category_id_df[['category_id', 'Product']].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "485953df-b3ec-44d5-9f5b-e2fd529382be",
      "metadata": {
        "id": "485953df-b3ec-44d5-9f5b-e2fd529382be"
      },
      "outputs": [],
      "source": [
        "# failed, scikit-learn 1.2.2\n",
        "from sklearn.feature_selection import chi2\n",
        "import numpy as np\n",
        "\n",
        "N = 2\n",
        "for Product, category_id in sorted(category_to_id.items()):\n",
        "  features_chi2 = chi2(features, labels == category_id)\n",
        "  indices = np.argsort(features_chi2[0])\n",
        "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
        "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
        "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
        "  print(\"# '{}':\".format(Product))\n",
        "  print(\"  . Most correlated unigrams:\\n       . {}\".format('\\n       . '.join(unigrams[-N:])))\n",
        "  print(\"  . Most correlated bigrams:\\n       . {}\".format('\\n       . '.join(bigrams[-N:])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bfbd0c5-8171-40d8-9f08-84c054d5970d",
      "metadata": {
        "id": "1bfbd0c5-8171-40d8-9f08-84c054d5970d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5AO0RzftwUNr"
      },
      "id": "5AO0RzftwUNr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import beta\n",
        "\n",
        "class BetaTargetEncoder:\n",
        "    def __init__(self):\n",
        "        self.encoder = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for i, category in enumerate(np.unique(X)):\n",
        "            category_values = X[X == category]\n",
        "            target_values = y[X == category]\n",
        "            self.encoder[category] = beta.fit(target_values, [len(target_values), len(target_values) - len(target_values)])\n",
        "\n",
        "    def transform(self, X):\n",
        "        encoded_values = []\n",
        "        for category in np.unique(X):\n",
        "            category_values = X[X == category]\n",
        "            encoded_values.append(self.encoder[category].sf(category_values))\n",
        "        return np.array(encoded_values).T\n",
        "\n",
        "# Example usage:\n",
        "import numpy as np\n",
        "from scipy.stats import beta\n",
        "\n",
        "class BetaTargetEncoder:\n",
        "    def __init__(self):\n",
        "        self.encoder = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for i, category in enumerate(np.unique(X)):\n",
        "            category_values = X[X == category]\n",
        "            target_values = y[X == category]\n",
        "            self.encoder[category] = beta.fit(target_values, [len(target_values), len(target_values) - len(target_values)])\n",
        "\n",
        "    def transform(self, X):\n",
        "        encoded_values = []\n",
        "        for category in np.unique(X):\n",
        "            category_values = X[X == category]\n",
        "            encoded_values.append(self.encoder[category].sf(category_values))\n",
        "        return np.array(encoded_values).T\n",
        "\n",
        "# Example usage:\n",
        "X2 = np.array(['A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'])\n",
        "y2 = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
        "\n",
        "bte = BetaTargetEncoder()\n",
        "bte.fit(X2, y2)\n",
        "encoded = bte.transform(X2)\n",
        "print(encoded)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zbwpe8iAwURQ"
      },
      "id": "Zbwpe8iAwURQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gmltme9nxBik"
      },
      "id": "Gmltme9nxBik",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GoHOThuVxX27"
      },
      "id": "GoHOThuVxX27",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appendix\n",
        "---\n",
        "1. Hyperparmeter"
      ],
      "metadata": {
        "id": "2Xzf3K1LWTI6"
      },
      "id": "2Xzf3K1LWTI6"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "metadata": {
        "id": "83kWkkEPWa86"
      },
      "id": "83kWkkEPWa86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "        'min_child_weight': [1, 5, 10],\n",
        "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'max_depth': [3, 4, 5]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "XVydXMW4Wkmz"
      },
      "id": "XVydXMW4Wkmz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb3 = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n",
        "                    silent=True, nthread=1)"
      ],
      "metadata": {
        "id": "HG45bkjWZFV1"
      },
      "id": "HG45bkjWZFV1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folds = 3\n",
        "param_comb = 5\n",
        "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 42)\n",
        "random_search = RandomizedSearchCV(xgb, param_distributions=params, \\\n",
        "                                   n_iter=param_comb, scoring='accuracy',\\\n",
        "                                   n_jobs=4,\n",
        "                                   cv=skf.split(X_train,y_train),\n",
        "                                   verbose=3, random_state=42 )"
      ],
      "metadata": {
        "id": "pRdEJXOPXodR"
      },
      "id": "pRdEJXOPXodR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_search.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "id": "oTRXrwTbYXvH"
      },
      "id": "oTRXrwTbYXvH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(random_search.best_score_)\n"
      ],
      "metadata": {
        "id": "6R847g8sYgZZ"
      },
      "id": "6R847g8sYgZZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "pprint(random_search.best_estimator_.get_params())"
      ],
      "metadata": {
        "id": "Fy1JMtibZXXp"
      },
      "id": "Fy1JMtibZXXp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8YJ_4x0w1dkG"
      },
      "id": "8YJ_4x0w1dkG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Volting\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Create a voting classifier\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('xgb', xgb),\n",
        "    ('rfc', rfc),\n",
        "    ('catboost', catboost),\n",
        "    ('lgbm', lgbm)], voting='hard')\n",
        "\n",
        "# Fit the voting classifier\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = voting_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "aTRHNb8H1dnc"
      },
      "id": "aTRHNb8H1dnc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacking\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb),\n",
        "        ('rfc', rfc),\n",
        "        ('catboost', catboost),\n",
        "        ('lgbm', lgbm)],\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "# Fit the stacking classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = stacking_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "zrahLLMl1ohw"
      },
      "id": "zrahLLMl1ohw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Blending\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Split the training data into two parts\n",
        "X_train_part1, X_train_part2, y_train_part1, y_train_part2 = train_test_split(X_train, y_train, test_size=0.5, random_state=42)\n",
        "\n",
        "# Initialize the models\n",
        "xgb = XGBClassifier()\n",
        "rfc = RandomForestClassifier()\n",
        "catboost = CatBoostClassifier(verbose=0)\n",
        "lgbm = LGBMClassifier()\n",
        "\n",
        "# Fit the models on the first part of the training data\n",
        "xgb.fit(X_train_part1, y_train_part1)\n",
        "rfc.fit(X_train_part1, y_train_part1)\n",
        "catboost.fit(X_train_part1, y_train_part1)\n",
        "lgbm.fit(X_train_part1, y_train_part1)\n",
        "\n",
        "# Generate predictions on the second part of the training data\n",
        "preds_part2_xgb = xgb.predict_proba(X_train_part2)[:, 1]\n",
        "preds_part2_rfc = rfc.predict_proba(X_train_part2)[:, 1]\n",
        "preds_part2_catboost = catboost.predict_proba(X_train_part2)[:, 1]\n",
        "preds_part2_lgbm = lgbm.predict_proba(X_train_part2)[:, 1]\n",
        "\n",
        "# Create a new dataset using these predictions as features\n",
        "X_train_meta = np.column_stack((preds_part2_xgb, preds_part2_rfc, preds_part2_catboost, preds_part2_lgbm))\n",
        "\n",
        "# Train the meta-model\n",
        "meta_model = LogisticRegression()\n",
        "meta_model.fit(X_train_meta, y_train_part2)\n",
        "\n",
        "# Generate predictions for the test set using the base models\n",
        "preds_test_xgb = xgb.predict_proba(X_test)[:, 1]\n",
        "preds_test_rfc = rfc.predict_proba(X_test)[:, 1]\n",
        "preds_test_catboost = catboost.predict_proba(X_test)[:, 1]\n",
        "preds_test_lgbm = lgbm.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Create a new dataset for the test set using these predictions as features\n",
        "X_test_meta = np.column_stack((preds_test_xgb, preds_test_rfc, preds_test_catboost, preds_test_lgbm))\n",
        "\n",
        "# Make final predictions using the meta-model\n",
        "y_pred = meta_model.predict(X_test_meta)\n"
      ],
      "metadata": {
        "id": "LjzKD8Mm1ok8"
      },
      "id": "LjzKD8Mm1ok8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Create a voting classifier\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('xgb', xgb),\n",
        "    ('rfc', rfc),\n",
        "    ('catboost', catboost),\n",
        "    ('lgbm', lgbm)], voting='soft')  # Use 'hard' for hard voting\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the voting classifier\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "id": "gFPeWhYX2rKV"
      },
      "id": "gFPeWhYX2rKV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "\n",
        "\n",
        "# Initialize the meta-model\n",
        "meta_model = LogisticRegression()\n",
        "\n",
        "# Create the stacking classifier\n",
        "stacking_clf = StackingClassifier(classifiers=[xgb, rfc, catboost, lgbm], meta_classifier=meta_model)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the stacking classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = stacking_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "Yp997C8yaER8"
      },
      "id": "Yp997C8yaER8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from pycaret.classification import *\n",
        "\n",
        "# Initialize the PyCaret setup\n",
        "clf1 = setup(data, target='churn')\n",
        "\n",
        "# Compare models and select top models for stacking\n",
        "top3 = compare_models(n_select=3)\n",
        "\n",
        "# Create a stack of models\n",
        "stacker = stack_models(estimator_list=top3, meta_model=LogisticRegression())\n",
        "\n",
        "# Finalize the model\n",
        "final_stacker = finalize_model(stacker)\n",
        "\n",
        "# Predict on new data\n",
        "predictions = predict_model(final_stacker, data=test_data)\n"
      ],
      "metadata": {
        "id": "ea67SAzZ2n_8"
      },
      "id": "ea67SAzZ2n_8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from vecstack import StackingTransformer\n",
        "\n",
        "# Initialize the base models\n",
        "models = [\n",
        "    ('xgb', XGBClassifier()),\n",
        "    ('rfc', RandomForestClassifier()),\n",
        "    ('catboost', CatBoostClassifier(verbose=0)),\n",
        "    ('lgbm', LGBMClassifier())\n",
        "]\n",
        "\n",
        "# Initialize the stacking transformer\n",
        "stack = StackingTransformer(estimators=models, regression=False, verbose=2, random_state=42)\n",
        "\n",
        "# Fit the stacking transformer\n",
        "stack.fit(X_train, y_train)\n",
        "\n",
        "# Transform the training set\n",
        "X_train_transformed = stack.transform(X_train)\n",
        "\n",
        "# Initialize and fit the meta-model\n",
        "meta_model = LogisticRegression()\n",
        "meta_model.fit(X_train_transformed, y_train)\n",
        "\n",
        "# Transform the test set\n",
        "X_test_transformed = stack.transform(X_test)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = meta_model.predict(X_test_transformed)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "id": "l_KXNwcK3ALf"
      },
      "id": "l_KXNwcK3ALf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement\n",
        "---\n",
        "Implement the model result on the cloud is the final step of procedures. Thought we do not introduce details here, only outline was described:\n",
        "1. prepare the model output, model.pkl for instance, and create the Python code with streamlit package for inputing the data:\n",
        "   - use"
      ],
      "metadata": {
        "id": "MOaL2ucT1qE6"
      },
      "id": "MOaL2ucT1qE6"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}